{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_A1SjL13URUM-gpyGlgt8tukjX_DeHg3",
      "authorship_tag": "ABX9TyPaz57b/slE9L95WA3Xlly5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgetsova/kaggle/blob/main/IceCube_preds_on_trained_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R79-Od_y7VFo",
        "outputId": "1107f3a2-01bf-481e-d3ce-3f204aeae039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.9/dist-packages (0.1.22)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (from opendatasets) (1.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from opendatasets) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from opendatasets) (8.1.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2.27.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.5.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "# Helper functions for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "!pip install opendatasets\n",
        "!pip install pandas\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, List, Optional, Sequence, Tuple, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from drive.MyDrive.IceCube.ice_transparency import ice_transparency\n",
        "from drive.MyDrive.IceCube.prepare_sensors import prepare_sensors\n",
        "\n",
        "INPUT_PATH = Path(\"drive/MyDrive/IceCube\")\n",
        "TRANSPARENCY_PATH = INPUT_PATH / \"ice_transperancy.txt\"\n",
        "#FULL_TRAIN_META_PATH = INPUT_PATH / \"train_meta.parquet\"\n",
        "TRAIN_PATH = INPUT_PATH / \"train\"\n",
        "STARTER_PULSE_PATH = TRAIN_PATH / 'pulses'\n",
        "STARTER_META_PATH = TRAIN_PATH / 'meta'\n",
        "\n",
        "BATCH_FILE = 2\n",
        "\n",
        "meta = pd.read_parquet(STARTER_META_PATH / f\"batch_{BATCH_FILE}.parquet\", columns=[\"event_id\", 'azimuth', 'zenith'], engine=\"pyarrow\", use_threads=True)\n",
        "sensor_df = prepare_sensors(INPUT_PATH / \"sensor_geometry.csv\")\n",
        "f_scattering, f_absorption = ice_transparency(TRANSPARENCY_PATH)\n",
        "event_ids = meta.event_id\n",
        "y = meta[['zenith', 'azimuth']].reset_index(drop=True)\n",
        "\n",
        "import torch\n",
        "#from torch.utils.data import Dataset\n",
        "from torch_geometric.nn import knn_graph\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class IceCubeDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        batch_id,\n",
        "        event_ids,\n",
        "        PATH_TO_BATCH_FILES,\n",
        "        f_scattering,\n",
        "        f_absorption,\n",
        "        sensor_df,\n",
        "        y,\n",
        "        pulse_limit=300,\n",
        "        transform=None,\n",
        "        pre_transform=None,\n",
        "        pre_filter=None,\n",
        "    ):\n",
        "        super().__init__(transform, pre_transform, pre_filter)\n",
        "        self.event_ids = event_ids\n",
        "        self.batch_df = pd.read_parquet(PATH_TO_BATCH_FILES / f\"batch_{batch_id}.parquet\")\n",
        "        self.sensor_df = sensor_df\n",
        "        self.pulse_limit = pulse_limit\n",
        "        self.f_scattering = f_scattering\n",
        "        self.f_absorption = f_absorption\n",
        "        self.y = y\n",
        "        #ice_transparency(TRANSPARENCY_PATH)\n",
        "\n",
        "        self.batch_df[\"time\"] = (self.batch_df[\"time\"] - 1.0e04) / 3.0e4\n",
        "        self.batch_df[\"charge\"] = np.log10(self.batch_df[\"charge\"]) / 3.0\n",
        "        self.batch_df[\"auxiliary\"] = self.batch_df[\"auxiliary\"].astype(int) - 0.5\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.event_ids)\n",
        "\n",
        "    def get(self, idx):\n",
        "        event_id = self.event_ids[idx]\n",
        "        event = self.batch_df.loc[event_id]\n",
        "\n",
        "        event = pd.merge(event, self.sensor_df, on=\"sensor_id\")\n",
        "\n",
        "        x = event[[\"x\", \"y\", \"z\", \"time\", \"charge\", \"qe\", \"auxiliary\"]].values\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "        data = Data(x=x, n_pulses=torch.tensor(x.shape[0], dtype=torch.int32))\n",
        "\n",
        "        # Add ice transparency data\n",
        "        z = data.x[:, 2].numpy()\n",
        "        scattering = torch.tensor(self.f_scattering(z), dtype=torch.float32).view(-1, 1)\n",
        "        # absorption = torch.tensor(self.f_absorption(z), dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "        data.x = torch.cat([data.x, scattering], dim=1)\n",
        "\n",
        "        # Downsample the large events\n",
        "        if data.n_pulses > self.pulse_limit:\n",
        "            data.x = data.x[np.random.choice(data.n_pulses, self.pulse_limit)]\n",
        "            data.n_pulses = torch.tensor(self.pulse_limit, dtype=torch.int32)\n",
        "\n",
        "         # Builds graph from the k-nearest neighbours.\n",
        "        data.edge_index = knn_graph(\n",
        "            data.x[:, [0, 1, 2]],  # x, y, z\n",
        "            k=8,\n",
        "            batch=None,\n",
        "            loop=False\n",
        "        )\n",
        "\n",
        "        if self.y is not None:\n",
        "            y = self.y.loc[idx, :].values\n",
        "            y = torch.tensor(y, dtype=torch.float32)\n",
        "            data.y = y\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCtQZ2kJGOv4",
        "outputId": "9b2d21ed-ff03-414d-82a4-9f14252ae28c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = IceCubeDataset(BATCH_FILE, list(event_ids), STARTER_PULSE_PATH, f_scattering, f_absorption, sensor_df, y)\n",
        "dataset.len()\n",
        "dataset.get(0).x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLQBnOgeGNdm",
        "outputId": "49c62207-88b1-4965-f1a0-9938ad7579a9"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([74, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils.homophily import homophily\n",
        "\n",
        "def calculate_xyzt_homophily(x, edge_index, batch):\n",
        "    \"\"\"Calculate xyzt-homophily from a batch of graphs.\n",
        "\n",
        "    Homophily is a graph scalar quantity that measures the likeness of\n",
        "    variables in nodes. Notice that this calculator assumes a special order of\n",
        "    input features in x.\n",
        "\n",
        "    Returns:\n",
        "        Tuple, each element with shape [batch_size,1].\n",
        "    \"\"\"\n",
        "    hx = homophily(edge_index, x[:, 0], batch).reshape(-1, 1)\n",
        "    hy = homophily(edge_index, x[:, 1], batch).reshape(-1, 1)\n",
        "    hz = homophily(edge_index, x[:, 2], batch).reshape(-1, 1)\n",
        "    ht = homophily(edge_index, x[:, 3], batch).reshape(-1, 1)\n",
        "    return hx, hy, hz, ht\n",
        "\n",
        "\n",
        "from torch_geometric.nn import EdgeConv\n",
        "from torch_geometric.nn.pool import knn_graph # differtent from one above????\n",
        "from typing import Any, Callable, List, Optional, Sequence, Tuple, Union\n",
        "from torch import LongTensor, Tensor\n",
        "from torch_geometric.typing import Adj\n",
        "\n",
        "class DynEdgeConv(EdgeConv):\n",
        "    \"\"\"Dynamical edge convolution layer.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        nn: Callable,\n",
        "        device,\n",
        "        aggr: str = \"max\",\n",
        "        nb_neighbors: int = 8,\n",
        "        features_subset: Optional[Union[Sequence[int], slice]] = None,\n",
        "        **kwargs: Any,\n",
        "    ):\n",
        "        \"\"\"Construct `DynEdgeConv`.\n",
        "        Args:\n",
        "            nn: The MLP/torch.Module to be used within the `EdgeConv`.\n",
        "            aggr: Aggregation method to be used with `EdgeConv`.\n",
        "            nb_neighbors: Number of neighbours to be clustered after the\n",
        "                `EdgeConv` operation.\n",
        "            features_subset: Subset of features in `Data.x` that should be used\n",
        "                when dynamically performing the new graph clustering after the\n",
        "                `EdgeConv` operation. Defaults to all features.\n",
        "            **kwargs: Additional features to be passed to `EdgeConv`.\n",
        "        \"\"\"\n",
        "        # Check(s)\n",
        "        if features_subset is None:\n",
        "            features_subset = slice(None)  # Use all features\n",
        "        assert isinstance(features_subset, (list, slice))\n",
        "\n",
        "        # Base class constructor\n",
        "        super().__init__(nn=nn, aggr=aggr, **kwargs)\n",
        "\n",
        "        # Additional member variables\n",
        "        self.device = device\n",
        "        self.nb_neighbors = nb_neighbors\n",
        "        self.features_subset = features_subset\n",
        "\n",
        "    def forward(\n",
        "        self, x: Tensor, edge_index: Adj, batch: Optional[Tensor] = None\n",
        "    ) -> Tensor:\n",
        "\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        # Standard EdgeConv forward pass\n",
        "        x = super().forward(x, edge_index)\n",
        "        # Recompute adjacency\n",
        "        edge_index = knn_graph(x=x[:, self.features_subset], k=self.nb_neighbors, \n",
        "                               batch=batch).to(self.device)\n",
        "\n",
        "        return x, edge_index\n",
        "\n",
        "def define_mlp(nb_in, nb_hidden, nb_out, activation):\n",
        "    return torch.nn.Sequential(torch.nn.Linear(nb_in, nb_hidden),\n",
        "                               activation,\n",
        "                               torch.nn.Linear(nb_hidden, nb_out),\n",
        "                               activation)\n",
        "    \n",
        "import torch.nn as nn\n",
        "from torch_scatter import scatter_max, scatter_mean, scatter_min, scatter_sum\n",
        "\n",
        "GLOBAL_POOLINGS = {\n",
        "    \"min\": scatter_min,\n",
        "    \"max\": scatter_max,\n",
        "    \"sum\": scatter_sum,\n",
        "    \"mean\": scatter_mean,\n",
        "}\n",
        "\n",
        "class MyGNN(nn.Module):\n",
        "    def __init__(self, nb_inputs, features_subset, device = 'cpu', nb_neighbors = 8, global_pooling_schemes = ['mean', 'min', 'max', 'sum'], \n",
        "                 add_global_variables_after_pooling = True, num_global_variables = 5):\n",
        "        super().__init__()\n",
        "        self.activation = torch.nn.LeakyReLU()\n",
        "        self.device = device\n",
        "\n",
        "        self.conv_layer1 = DynEdgeConv(define_mlp(nb_inputs*2, 128, 256, self.activation), self.device, aggr=\"add\", nb_neighbors=8, features_subset=features_subset)\n",
        "        self.conv_layer2 = DynEdgeConv(define_mlp(256*2, 336, 256, self.activation), self.device, aggr=\"add\", nb_neighbors=8, features_subset=features_subset)\n",
        "        self.conv_layer3 = DynEdgeConv(define_mlp(256*2, 336, 256, self.activation), self.device, aggr=\"add\", nb_neighbors=8, features_subset=features_subset)\n",
        "        self.conv_layer4 = DynEdgeConv(define_mlp(256*2, 336, 256, self.activation), self.device, aggr=\"add\", nb_neighbors=8, features_subset=features_subset)\n",
        "\n",
        "        self.post_processing = define_mlp(256*4 + nb_inputs, 336, 256, self.activation)\n",
        "\n",
        "        nb_poolings = (len(global_pooling_schemes) if global_pooling_schemes else 1)\n",
        "        nb_latent_features = 256 * nb_poolings\n",
        "        if add_global_variables_after_pooling:\n",
        "            nb_latent_features += num_global_variables\n",
        "\n",
        "        #self.readout = torch.nn.Sequential(torch.nn.Linear(nb_latent_features, 128), self.activation)\n",
        "        self.readout = define_mlp(nb_latent_features, 128, 2, self.activation)\n",
        "        \n",
        "        self._global_pooling_schemes = global_pooling_schemes\n",
        "        self._add_global_variables_after_pooling = add_global_variables_after_pooling\n",
        "\n",
        "    def _global_pooling(self, x: Tensor, batch: LongTensor) -> Tensor:\n",
        "      \"\"\"Perform global pooling.\"\"\"\n",
        "      assert self._global_pooling_schemes\n",
        "      pooled = []\n",
        "      for pooling_scheme in self._global_pooling_schemes:\n",
        "        pooling_fn = GLOBAL_POOLINGS[pooling_scheme]\n",
        "        pooled_x = pooling_fn(x, index=batch, dim=0)\n",
        "        if isinstance(pooled_x, tuple) and len(pooled_x) == 2:\n",
        "          pooled_x, _ = pooled_x\n",
        "        pooled.append(pooled_x)\n",
        "\n",
        "      return torch.cat(pooled, dim=1)\n",
        "\n",
        "    def _calculate_global_variables(self, x: Tensor, edge_index: LongTensor, batch: LongTensor, \n",
        "                                    *additional_attributes: Tensor,) -> Tensor:\n",
        "      \"\"\"Calculate global variables.\"\"\"\n",
        "      # Calculate homophily (scalar variables)\n",
        "      h_x, h_y, h_z, h_t = calculate_xyzt_homophily(x, edge_index, batch)\n",
        "      # Calculate mean features\n",
        "      global_means = scatter_mean(x, batch, dim=0)\n",
        "      # Add global variables\n",
        "      # global_variables = torch.cat([global_means, h_x, h_y, h_z, h_t] + [attr.unsqueeze(dim=1) for attr in additional_attributes], dim=1,)\n",
        "      global_variables = torch.cat([h_x, h_y, h_z, h_t] + [attr.unsqueeze(dim=1) for attr in additional_attributes], dim=1,)\n",
        "\n",
        "\n",
        "      return global_variables\n",
        "\n",
        "    def forward(self, data: Data) -> Tensor:\n",
        "      \"\"\"Apply learnable forward pass.\"\"\"\n",
        "      # Convenience variables\n",
        "      x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "      global_variables = self._calculate_global_variables(x, edge_index, \n",
        "                                                          batch, torch.log10(data.n_pulses))\n",
        "\n",
        "      # Distribute global variables out to each node\n",
        "      if not self._add_global_variables_after_pooling:\n",
        "        distribute = (batch.unsqueeze(dim=1) == torch.unique(batch).unsqueeze(dim=0)).type(torch.float) \n",
        "        global_variables_distributed = torch.sum(distribute.unsqueeze(dim=2) * \n",
        "                                                 global_variables.unsqueeze(dim=0), dim=1)\n",
        "        x = torch.cat((x, global_variables_distributed), dim=1)\n",
        "\n",
        "      # convolutions\n",
        "      state_graphs = [x]\n",
        "      conv_layers = [self.conv_layer1, self.conv_layer2, self.conv_layer3, self.conv_layer4]\n",
        "      for layer in conv_layers:\n",
        "        x, edge_index = layer(x, edge_index, batch)\n",
        "        state_graphs.append(x)\n",
        "\n",
        "      x = torch.cat(state_graphs, dim=1)\n",
        "      # post processing\n",
        "      x = self.post_processing(x)\n",
        "\n",
        "      # global pooling\n",
        "      if self._global_pooling_schemes:\n",
        "        x = self._global_pooling(x, batch=batch)\n",
        "        if self._add_global_variables_after_pooling:\n",
        "          x = torch.cat([x, global_variables], dim=1)\n",
        "\n",
        "      # readout layer\n",
        "      x = self.readout(x)\n",
        "      return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "compute_knn_with = [0, 1, 2, 3] # x, y, z, time\n",
        "model = MyGNN(8, compute_knn_with, device='cuda')\n",
        "print('using ', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVoryTIx87Oc",
        "outputId": "20d9d34e-c10f-40dd-b7e3-7d9a2ff51d81"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using  cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def angular_dist_score(az_true, zen_true, az_pred, zen_pred):\n",
        "    '''\n",
        "    calculate the MAE of the angular distance between two directions.\n",
        "    The two vectors are first converted to cartesian unit vectors,\n",
        "    and then their scalar product is computed, which is equal to\n",
        "    the cosine of the angle between the two vectors. The inverse \n",
        "    cosine (arccos) thereof is then the angle between the two input vectors\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    \n",
        "    az_true : float (or array thereof)\n",
        "        true azimuth value(s) in radian\n",
        "    zen_true : float (or array thereof)\n",
        "        true zenith value(s) in radian\n",
        "    az_pred : float (or array thereof)\n",
        "        predicted azimuth value(s) in radian\n",
        "    zen_pred : float (or array thereof)\n",
        "        predicted zenith value(s) in radian\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    \n",
        "    dist : float\n",
        "        mean over the angular distance(s) in radian\n",
        "    '''\n",
        "    \n",
        "    if not (np.all(np.isfinite(az_true)) and\n",
        "            np.all(np.isfinite(zen_true)) and\n",
        "            np.all(np.isfinite(az_pred)) and\n",
        "            np.all(np.isfinite(zen_pred))):\n",
        "        raise ValueError(\"All arguments must be finite\")\n",
        "    \n",
        "    # pre-compute all sine and cosine values\n",
        "    sa1 = np.sin(az_true)\n",
        "    ca1 = np.cos(az_true)\n",
        "    sz1 = np.sin(zen_true)\n",
        "    cz1 = np.cos(zen_true)\n",
        "    \n",
        "    sa2 = np.sin(az_pred)\n",
        "    ca2 = np.cos(az_pred)\n",
        "    sz2 = np.sin(zen_pred)\n",
        "    cz2 = np.cos(zen_pred)\n",
        "    \n",
        "    # scalar product of the two cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
        "    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n",
        "    \n",
        "    # scalar product of two unit vectors is always between -1 and 1, this is against nummerical instability\n",
        "    # that might otherwise occure from the finite precision of the sine and cosine functions\n",
        "    scalar_prod =  np.clip(scalar_prod, -1, 1)\n",
        "    \n",
        "    # convert back to an angle (in radian)\n",
        "    return np.average(np.abs(np.arccos(scalar_prod)))\n",
        " "
      ],
      "metadata": {
        "id": "DCAZcyjr-OLq"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_loader = DataLoader(dataset, batch_size=1, num_workers=1)"
      ],
      "metadata": {
        "id": "vn863HTFL2nZ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = IceCubeDataset(BATCH_FILE, event_ids, STARTER_PULSE_PATH, f_scattering, f_absorption, sensor_df, y)\n",
        "MODEL_PATH = INPUT_PATH / \"first_model_epoch0.pt\"\n",
        "model = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
        "\n",
        "def infer(model, loader, device=\"cpu\"):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      #preds = model(loader)\n",
        "      #for batch in tqdm(loader, desc=\"test\"): \n",
        "      #for batch_idx, data in enumerate(loader):\n",
        "      for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        preds = model(batch)\n",
        "\n",
        "    return preds\n",
        "\n",
        "infer(model, check_loader, device='cpu')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "6RJeEvaV8ONd",
        "outputId": "c7842ab8-ec87-4200-bf1b-7ee082ef2e3c"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-9598f96bd6f1>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-77-9598f96bd6f1>\u001b[0m in \u001b[0;36minfer\u001b[0;34m(model, loader, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-35ca8f88c2a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0mconv_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mstate_graphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-35ca8f88c2a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Recompute adjacency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         edge_index = knn_graph(x=x[:, self.features_subset], k=self.nb_neighbors, \n\u001b[0m\u001b[1;32m     71\u001b[0m                                batch=batch).to(self.device)\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_MODULE_LOADING'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'LAZY'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    }
  ]
}